---
title: 大模型知识之一——基础架构
description:
date: 2025-11-23T15:47:55+08:00
image: https://pic.imgdb.cn/item/65927ef2c458853aefd7b5ce.jpg
tags:
  - llm
categories:
  - 技术
math: true
license:
hidden: false
comments: true
draft: false
toc: true
lastmod: 2026-02-21
---
# 基础架构

![Transformer,_full_architecture.png](https://image-1258996033.cos.ap-shanghai.myqcloud.com/Transformer%2C_full_architecture.png?imageSlim)
上图是transfomer block的基础架构图，由标准的encoder和decoder的结构组成，但是在chatgpt里面仅仅包含decoder部分的结构，所以我们仅仅专注于右边部分的结构。GPT2的网络结构如下所示

```python
GPT2Model(
  (wte): Embedding(50257, 768)
  (wpe): Embedding(1024, 768)
  (drop): Dropout(p=0.1, inplace=False)
  (h): ModuleList(
    (0-11): 12 x GPT2Block(
      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): GPT2Attention(
        (c_attn): Conv1D()
        (c_proj): Conv1D()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (resid_dropout): Dropout(p=0.1, inplace=False)
      )
      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): GPT2MLP(
        (c_fc): Conv1D()
        (c_proj): Conv1D()
        (act): NewGELUActivation()
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
  )
  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
)
Total # of params: 124.44M
```

Tansformer架构核心氛围如下几步，

1. 将所有的语料token数字化，将人类的语言转换成便于计算机处理的数字信息，得到全量的token表格；
2. 输入一段token，加上位置编码的向量，在token中加入位置信息
3. 利用全中矩阵计算每个token的q/k/v向量，计算每一个token与其他token的相关性（通过计算当前token向量的q向量与其他token的k向量的内积得到），根据相关性得到权重系数（通过softmax获得），然后计算每一个token的v向量的加权和，更新当前token的v向量；
4. 得到多头的v向量，然后使用投影矩阵将其映射到与第2步相同大小的向量，两个相加之后做layernorm；
5. 使用权重矩阵将第4步的向量升维，然后做激活函数处理，再降维到之前的第2步的大小；
6. 上面的2~5步就是一个完整的transformer block的过程，重复多次之后就可以通过现行层得到最少的logits softmax的矩阵，通过最后一列（最后一个token）的最大值从全量的token表格中取出对应编号的token，即为下一个输出的token
7. 将上面的第6步生成的token再加入到原来的token序列的尾巴上，送到transoformer的网络中重复步骤2~7

下面以输入6个token为例，通过画图说明上面的所有过程。GPT-2的参数如下，全部token一共有50257个，每个token的嵌入维度是768，可以处理的最大连续的token数字是1024.

## 位置编码

![image.png](https://image-1258996033.cos.ap-shanghai.myqcloud.com/20251123185902.png?imageSlim)

当前有6个token的输入，每个长度是768的一维向量 $\{a_0, a_1,...,a_5\}$，位置编码的向量的长度也是768, $\{p_0, p_1,...,p_5\}$，相加之后得到位置编码之后的向量

$$
a_i^{'} = a_i + p_i
$$

其中 $a_i^{'},a_i,p_i \in R^{1\times 768}$

## Masked MHA

![image.png](https://image-1258996033.cos.ap-shanghai.myqcloud.com/20251123190659.png?imageSlim)

在这一步之前，有些大模型中不需要对每个token做layernorm的归一化处理，GPT-2中需要做这个处理，做完之后可以得到归一化之后的 $a{}'$，接着计算对应的q/k/v向量，

$$
\left\{\begin{matrix}
  q_i=a_i{}'W^{Q} + b^{Q} \\
  k_i=a_i{}'W^{K} + b^{K} \\
  v_i=a_i{}'W^{V} + b^{V}
\end{matrix}\right.
$$

上面示意图中的维度有问题，GPT-2中有12个头，所以每个头计算完成之后的二q/k/v向量的长度是768/12= 64，因此 $q_i,k_i,v_i\in R^{1\times 64}$ ，而 $W^{Q}, W^{K},W^V \in R^{768\times 64}$。得到所有token的QKV矩阵如下

$$
Q = \begin{bmatrix}
  q_0 \\
  q_1 \\
  \vdots \\
  q_5 \\
\end{bmatrix} ,
K = \begin{bmatrix}
  k_0 \\
  k_1 \\
  \vdots \\
  k_5 \\
\end{bmatrix} ,
V = \begin{bmatrix}
  v_0 \\
  v_1 \\
  \vdots \\
  v_5 \\
\end{bmatrix}
$$

其中 $Q,K,V \in R^{6\times 64}$。下一步计算不同token之间的相关性，计算第i个token跟第j个token相关性就是计算 $q_ik^T_j$, 得到如下的**自注意力矩阵**，

![image.png](https://image-1258996033.cos.ap-shanghai.myqcloud.com/20251123192138.png?imageSlim)

$$
A = \begin{bmatrix}
  q_0k_0^T & q_0k_1^T & \dots & q_0k_5^T     \\
q_1k_0^T & q_1k_1^T & \dots & q_1k_5^T     \\
  \vdots & \vdots &\dots &\vdots \\
  q_5k_0^T & q_5k_1^T & \dots & q_5k_5^T     \\
\end{bmatrix}  =\begin{bmatrix}
  q_0 \\
  q_1 \\
  \vdots \\
  q_5 \\
\end{bmatrix}\begin{bmatrix}
  k_0 &
  k_1 &
  \dots &
  k_5 \\
\end{bmatrix} ,= QK^T
$$

其实 $A\in \mathbb{R}^{6\times 6}$，下一步是masked softmax，因为大模型推理过程只能从已知的token推测之后的token，所以第 $i$ 个token只能知道前面的 $i$ 个token的信息，也就是说第 $j$ 个token只能计算前 $j - 1$ 个token的相关系数，所以上面的矩阵需要改成一个下三角矩阵

$$
A_m = A + Mask =  \begin{bmatrix}
  q_0k_0^T & -\infty & \dots & -\infty      \\
q_1k_0^T & q_1k_1^T & \dots & -\infty      \\
  \vdots & \vdots &\dots &\vdots \\
  q_5k_0^T & q_5k_1^T & \dots & q_5k_5^T     \\
\end{bmatrix}
$$

对上面的矩阵按照行做softmax，可以得到一个下三角矩阵，右上部分全部是0，

$$
A_{softmax} = \text{Softmax}(A_m) = \text{Softmax}(\frac{QK^T}{\sqrt{d}})
$$

下面的关键一步，是更新所有token的v向量，令

$$
v_i' = \sum_{j}A_{softmax}[i, j]v_j
$$

如下图所示

![image.png](https://image-1258996033.cos.ap-shanghai.myqcloud.com/20251123201405.png?imageSlim)

所以得到的新的v矩阵就是

$$
V' = \text{Softmax}(\frac{QK^T}{\sqrt{d}})V
$$

从上面的计算过程可以看到，新的value矩阵已经包括了其他token的相关性的信息，每个token都包括了其他token的信息。因为GPT-2有12个头，每个头计算得到相同维度的 $V'$ 矩阵，这些矩阵的数据是相互独立的，所以可以大规模的**并行计算**。得到12个新的V矩阵之后，拼接起来就可以得到更大的新矩阵，

$$
V_{new}' = [V_0',V_1',\dots,V_{11}']
$$

所以 $V_{new}'\in \mathbb{R}^{6\times 768}$，下一步再做一个矩阵的投影，使用矩阵 $W_{prj} \in \mathbb{R}^{768\times 768}$，得到新的V矩阵

$$
V_{final}' = V_{new}'W_{prj} + b_{prj}
$$

最后得到的矩阵大小不变。

## 残差层和归一化层

计算完上面的步骤之后，做残差相加之后做layernorm

$$
L = Layernorm(V_{final}' + \begin{bmatrix}a_0' \\a_1'\\\vdots\\a_5'  \end{bmatrix})
$$

其中 $L \in \mathbb{R}^{6\times 768}$。

## MLP/FFN层

![image.png](https://image-1258996033.cos.ap-shanghai.myqcloud.com/20251123203454.png?imageSlim)

可以用如下的数学公式表示

$$
FFN(x) = Act(LW_1 + b_1)W_2
$$

其中  $W_1 \in \mathbb{R}^{768\times 3072}, W_2 \in \mathbb{R}^{3072\times 768}$，最后得到结果还是6个长度是768的token向量。

## Linear和Softmax层

在上面的所有的block重复12次之后，最后一个transformer block结束之后，再做一个layernorm，使用权重矩阵将计算结果映射到整个词汇表上，得到

$$
Softmax(B) = Softmax(V_{new}'W_f)
$$

其中 $W_f\in \mathbb{R}^{768\times 50257}$，最后一个token对应的那一行中的最大值的id，就是下一个输出的token。

# 参考文献

- [LLM Visualization](https://bbycroft.net/llm)
- [Transformer Explainer: LLM Transformer Model Visually Explained](https://poloclub.github.io/transformer-explainer/)
- [Transformer Math (Part 1) - Counting Model Parameters](https://michaelwornow.net/2024/01/18/counting-params-in-transformer)

---

本文原载于 [巴巴变的博客](http://blog.bugxch.top)，遵循CC BY-NC-SA 4.0协议，复制请保留原文出处。
